# Machine learning

## Notes

- A big part of the utility of math (especially in ML) is having breadth rather than depth. The strategy of picking out specific things you don't know from papers and looking them up is only effective if you have the breadth in your background to understand the answers you find.
  - Broad knowledge is also what helps you manage the exponential tree of complexity you're encountering.
    - You won't have seen all the things you come across, but you'll develop the ability to make good judgements about what you need to read to achieve your goals. You'll learn how to recognize when a reference you're reading is more (or less) technical than you need, and how to search for something more appropriate. You'll also learn how and when you can use results without understanding the details.
  - Finally, as a general grad student strategy trying to learn everything just in time is not a path to success. Even if you had the perfect math oracle that you want it would be setting you up to be left behind. All the oracle gives you is the ability to catch up quickly to the ideas of others. Your job as a grad student is to generate new knowledge and to do that you need to seek things out on your own, not just follow along the latest trend. Part of your job is to go out hunting for ideas that your peers haven't found yet and bring them back to your field.
- [In supervised learning, you have a bunch of data, a specific question you want to answer, and access to the correct answer to many instances of that question. In unsupervised learning, you have a bunch of data points, and you want to find meaningful patterns in the structure of that data. In reinforcement learning, you have a task you want to take actions to accomplish, and you don't have any access to knowing what the best action is, but after each action you get a rough idea of how good the result was.](https://www.reddit.com/r/MachineLearning/comments/7780ok/r_alphago_zero_learning_from_scratch_deepmind/dol3knx/ "permalink")
- AI doesn't need to follow the human model, just like planes don't need to flap their wings like a bird. For most jobs AI will be very different from humans. Even when AI acts as human for entertainment I would imagine them being very different internally, as their job is to mimic aspects of human behaviors, not actually a human as a whole.
- Almost all of machine learning is about representing data as vectors and performing linear and non-linear transformations in order to perform classification, regression, etc.
- Most of ML is fitting models to data. To fit a model you minimize some error measure as a function of its real valued parameters, e.g. the weights of the connections in a neural network. The algorithms to do the minimization are based on gradient descent, which depends on derivatives, i.e. differential calculus.

## Links

- [What worries me about AI](https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704)
- [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
- [Ask HN: Best way to get started with AI?](https://news.ycombinator.com/item?id=15689399)
- [Ask HN: What maths are critical to pursuing ML/AI?](https://news.ycombinator.com/item?id=15116379)
- [Ask HN: 'Crash Courses' for Mathematics Related to DL, ML and Data Analysis](https://news.ycombinator.com/item?id=16508873)
- [Computational Statistics and Machine Learning Revision Notes](https://github.com/acbraith/CSML_notes)
- [Stanford CS229 course](https://github.com/econti/cs229)
- [Readings in applied data science](https://github.com/hadley/stats337)
- [Learn ML in 3 months](https://github.com/llSourcell/Learn_Machine_Learning_in_3_Months#readme)
- [Deep Learn](https://github.com/GauravBh1010tt/DeepLearn#readme) - Implementation of research papers on Deep Learning+ NLP+ CV in Python using Keras, TensorFlow and Scikit Learn.
- [Building Brundage Bot](https://hackernoon.com/building-brundage-bot-10252facf3d1)
- [Summaries of ML papers](https://github.com/aleju/papers)
- [Code and data for paper "Deep Painterly Harmonization"](https://github.com/luanfujun/deep-painterly-harmonization)
- [FB AI Tools](https://facebook.ai/developers/tools)
- [Best Practices for ML Engineering](https://developers.google.com/machine-learning/rules-of-ml/)
- [Machine Learning From Scratch](https://github.com/eriklindernoren/ML-From-Scratch#readme)
- [Dive Into ML](http://hangtwenty.github.io/dive-into-machine-learning/)
- [Fermat's Library NIPS comments](http://fermatslibrary.com/nips)
- [Heroes of Deep Learning: Andrew Ng interviews Ian Goodfellow](https://www.youtube.com/watch?v=pWAc9B2zJS4)
- [Machine Learning for Humans](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12) - Great article.
- [Deep Learning for Siri’s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis](https://machinelearning.apple.com/2017/08/06/siri-voices.html)
- [The Google Brain Team — Looking Back on 2017](https://ai.googleblog.com/2018/01/the-google-brain-team-looking-back-on.html?m=1)
- [Building the Software 2.0 Stack by Andrej Karpathy from Tesla (2018)](https://www.figure-eight.com/building-the-software-2-0-stack-by-andrej-karpathy-from-tesla/)
- [Deep Learning World](https://github.com/astorfi/Deep-Learning-World#readme)
- [Machine Learning cheatsheets for Stanford's CS 229](https://github.com/afshinea/stanford-cs-229-machine-learning#readme)
- [RAAIS 2018 - François Chollet (Creator of Keras)](https://www.youtube.com/watch?v=2L2u303FAs8)
- [KubeFlow](https://github.com/kubeflow/kubeflow#readme) - Machine Learning Toolkit for Kubernetes.
- [MIT AGI: Deep Learning (Yoshua Bengio) (2018)](https://www.youtube.com/watch?v=azOmzumh0vQ)
- [TL-GAN: transparent latent-space GAN](https://github.com/SummitKwan/transparent_latent_gan) - Use supervised learning to illuminate the latent space of GAN for controlled generation and edit.
- [Grokking Deep Learning](https://github.com/iamtrask/Grokking-Deep-Learning#readme) - Repository accompanying "Grokking Deep Learning" book.
- [HN: Can we rule out near-term AGI? (2018)](https://news.ycombinator.com/item?id=18405025)
- [Introduction to Grenade (Haskell library for Deep Learning)](https://www.huwcampbell.com/posts/2017-02-17-introduction-to-grenade.html)
- [Grenade](https://github.com/HuwCampbell/grenade) - Deep Learning in Haskell.
- [Deep Learning 1: Introduction to Machine Learning Based AI](https://www.youtube.com/watch?v=iOh7QUZGyiU)
- [Deep Learning cheatsheets for Stanford's CS 230 (2018)](https://github.com/afshinea/stanford-cs-230-deep-learning)
- [Deep Learning Book Chapter Summaries](https://github.com/dalmia/Deep-Learning-Book-Chapter-Summaries) - Attempting to make the Deep Learning Book easier to understand.
- [PracticalAI](https://github.com/GokuMohandas/practicalAI#readme) - Practical approach to learning machine learning.
- [Ask HN: How to incorporate machine learning into day job? (2018)](https://news.ycombinator.com/item?id=18650646)
- [RLgraph](https://github.com/rlgraph/rlgraph) - Flexible computation graphs for deep reinforcement learning.
- [Nevergrad](https://github.com/facebookresearch/nevergrad) - Gradient-free optimization platform.
- [Machine Learning Cheat Sheet](https://ml-cheatsheet.readthedocs.io/en/latest/)
- [GANs and Divergence Minimization (2018)](https://colinraffel.com/blog/gans-and-divergence-minimization.html)
- [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic) - Technical report on convolution arithmetic in the context of deep learning.
- [FloydHub](https://www.floydhub.com/) - Managed cloud platform for data scientists.
- [Style Transfer as Optimal Transport](https://github.com/VinceMarron/style_transfer) - Algorithm that transfers the distribution of visual characteristics, or style, of a reference image onto a subject image via an Optimal Transport plan.